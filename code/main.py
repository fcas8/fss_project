# from merge import merge_data
# from merge import merge_all_years
# from tokenizer_debug import process_tweets
from train_classifier import vectorize_train

# merge_data(2020)
# merge_data(2021)
# merge_data(2022)

# merge_all_years()

# process_tweets(sample_frac=.1, chunk_size=1000000)

vectorize_train()